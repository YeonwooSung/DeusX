{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzcuJMXHZMc7"
      },
      "source": [
        "A MinGPT + Lightning + xFormers example Code from Sean Naren (@seannaren)\n",
        "This is an hommage to https://github.com/karpathy/minGPT\n",
        "\n",
        "\n",
        "See https://github.com/facebookresearch/xformers/blob/main/examples/microGPT.py\n",
        "for a matching script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "gJlGTQ-lYERT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "586d3f15-80a1-4fc5-fe04-1ae622120df2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.13-cp38-cp38-manylinux_2_17_x86_64.whl (92.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 92.2 MB 28 kB/s \n",
            "\u001b[?25hCollecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.8.6-py3-none-any.whl (800 kB)\n",
            "\u001b[K     |████████████████████████████████| 800 kB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
            "Collecting pyre-extensions==0.0.23\n",
            "  Downloading pyre_extensions-0.0.23-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch>=1.12 in /usr/local/lib/python3.8/dist-packages (from xformers) (1.13.0+cu116)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from pyre-extensions==0.0.23->xformers) (4.4.0)\n",
            "Collecting typing-inspect\n",
            "  Downloading typing_inspect-0.8.0-py3-none-any.whl (8.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Collecting lightning-utilities!=0.4.0,>=0.3.0\n",
            "  Downloading lightning_utilities-0.5.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (2022.11.0)\n",
            "Collecting tensorboardX>=2.2\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 76.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.8/dist-packages (from pytorch_lightning) (21.3)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 74.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.8/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorboardX>=2.2->pytorch_lightning) (3.19.6)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.8/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.12.7)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, torchmetrics, tensorboardX, pyre-extensions, lightning-utilities, xformers, pytorch-lightning\n",
            "Successfully installed lightning-utilities-0.5.0 mypy-extensions-0.4.3 pyre-extensions-0.0.23 pytorch-lightning-1.8.6 tensorboardX-2.5.1 torchmetrics-0.11.0 typing-inspect-0.8.0 xformers-0.0.13\n"
          ]
        }
      ],
      "source": [
        "!pip install --pre torch\n",
        "!pip install xformers pytorch_lightning numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMQWysh7Y6nC"
      },
      "source": [
        "Now check all our dependencies. If Triton is not compatible with the GPU or the CUDA runtime served by Colab, please make sure that it's not installed in the above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fphnY4yrY9z_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe77b16-e2cc-48ad-f1de-95be65142d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:WARNING: /usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\n",
            "Need to compile C++ extensions to get sparse attention suport. Please run python setup.py build develop\n",
            "WARNING:root:A matching Triton is not available, some optimizations will not be enabled.\n",
            "Error caught was: No module named 'triton'\n",
            "WARNING:root:Triton is not available, some optimizations will not be enabled.\n",
            "Error No module named 'triton'\n",
            "WARNING:root:Triton is not available, FusedMLP will not be enabled.\n",
            "WARNING:root:Either FairScale or torch distributed is not available, MixtureOfExperts will not be exposed. Please install them if you would like to use MoE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/xformers/_C.so: undefined symbol: _ZNK3c104impl13OperatorEntry20reportSignatureErrorENS0_12CppSignatureE\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from pytorch_lightning.utilities import rank_zero_info\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
        "\n",
        "from xformers.factory.model_factory import xFormer, xFormerConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXTliQkHZI-6"
      },
      "source": [
        "Let's first define our GPT-like model. Please note that all the parameters in the config dictionnary can be changed more or less at will, but the attention mechanism needs to be compatible with causality constraints. We'll be using Pytorch Lightning to nicely specify all the specific training steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dQcFhB_jZXaH"
      },
      "outputs": [],
      "source": [
        "class GPT(pl.LightningModule):\n",
        "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        weight_decay=0.1,\n",
        "        betas=(0.9, 0.95),\n",
        "        learning_rate=1e-4,\n",
        "        n_embd=512,\n",
        "        block_size=128,\n",
        "        n_layer=8,\n",
        "        n_head=4,\n",
        "        resid_pdrop=0.1,\n",
        "        attn_pdrop=0.1,\n",
        "        mlp_pdrop=0.1,\n",
        "        attention=\"scaled_dot_product\",\n",
        "        hidden_layer_multiplier=4,\n",
        "        warmup_tokens=20,\n",
        "        final_tokens=1000,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # auto creates self.hparams from the method signature\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # A list of the encoder or decoder blocks which constitute the Transformer.\n",
        "        xformer_config = [\n",
        "            {\n",
        "                \"block_type\": \"encoder\",\n",
        "                \"num_layers\": self.hparams.n_layer,\n",
        "                \"dim_model\": self.hparams.n_embd,\n",
        "                \"residual_norm_style\": \"pre\",\n",
        "                \"position_encoding_config\": {\n",
        "                    \"name\": \"vocab\",\n",
        "                    \"seq_len\": self.hparams.block_size,\n",
        "                    \"vocab_size\": self.hparams.vocab_size,\n",
        "                },\n",
        "                \"multi_head_config\": {\n",
        "                    \"num_heads\": self.hparams.n_head,\n",
        "                    \"residual_dropout\": self.hparams.resid_pdrop,\n",
        "                    \"use_rotary_embeddings\": True,\n",
        "                    \"attention\": {\n",
        "                        \"name\": self.hparams.attention,\n",
        "                        \"dropout\": self.hparams.attn_pdrop,\n",
        "                        \"causal\": True,\n",
        "                        \"seq_len\": self.hparams.block_size,\n",
        "                    },\n",
        "                },\n",
        "                \"feedforward_config\": {\n",
        "                    \"name\": \"MLP\",\n",
        "                    \"dropout\": self.hparams.mlp_pdrop,\n",
        "                    \"activation\": \"gelu\",\n",
        "                    \"hidden_layer_multiplier\": self.hparams.hidden_layer_multiplier,\n",
        "                },\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        config = xFormerConfig(xformer_config)\n",
        "        self.model = xFormer.from_config(config)\n",
        "\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(self.hparams.n_embd)\n",
        "        self.head = nn.Linear(self.hparams.n_embd, self.hparams.vocab_size, bias=False)\n",
        "\n",
        "        self.block_size = self.hparams.block_size\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        self._tokens_seen = 0\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "        # Reset the token counter\n",
        "        self._tokens_seen = 0\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Create the optimizer and the training schedule:\n",
        "        # - Handle the per-param weight decay\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        params_decay = [\n",
        "            p for n, p in self.named_parameters() if not any(nd in n for nd in no_decay)\n",
        "        ]\n",
        "        params_nodecay = [\n",
        "            p for n, p in self.named_parameters() if any(nd in n for nd in no_decay)\n",
        "        ]\n",
        "        optim_groups = [\n",
        "            {\"params\": params_decay, \"weight_decay\": self.hparams.weight_decay},\n",
        "            {\"params\": params_nodecay, \"weight_decay\": 0.0},\n",
        "        ]\n",
        "\n",
        "        # - Start with a warm up, ramp up then cosine\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            optim_groups, lr=self.hparams.learning_rate, betas=self.hparams.betas\n",
        "        )\n",
        "\n",
        "        def update_lr(*_):\n",
        "            config = self.hparams\n",
        "\n",
        "            if self._tokens_seen < config.warmup_tokens:\n",
        "                # linear warmup\n",
        "                lr_mult = float(self._tokens_seen) / float(max(1, config.warmup_tokens))\n",
        "                lr_mult = max(lr_mult, 1e-2)  # could be that we've not seen any yet\n",
        "            else:\n",
        "                # cosine learning rate decay\n",
        "                progress = float(self._tokens_seen - config.warmup_tokens) / float(\n",
        "                    max(1, config.final_tokens - config.warmup_tokens)\n",
        "                )\n",
        "                lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "\n",
        "            return lr_mult\n",
        "\n",
        "        lr_scheduler = {\n",
        "            \"scheduler\": torch.optim.lr_scheduler.LambdaLR(\n",
        "                optimizer,\n",
        "                lr_lambda=[update_lr, update_lr],\n",
        "            ),\n",
        "            \"name\": \"learning_rate\",\n",
        "            \"interval\": \"step\",  # The unit of the scheduler's step size\n",
        "            \"frequency\": 1,  # The frequency of the scheduler\n",
        "        }\n",
        "        return [optimizer], [lr_scheduler]\n",
        "\n",
        "    def forward(self, src):\n",
        "        # predict the next tokens (in latent space)\n",
        "        prediction = self.model(src)\n",
        "\n",
        "        # translate the predictions into tokens\n",
        "        prediction = self.ln_f(prediction)\n",
        "        logits = self.head(prediction)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def training_step(self, batch, _):\n",
        "        src, targets = batch\n",
        "\n",
        "        # Update the tokens we've seen (tracked for LR scheduling)\n",
        "        self._tokens_seen += (src >= 0).numel()\n",
        "\n",
        "        # same action as inference\n",
        "        logits = self(src)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "\n",
        "        self.logger.log_metrics(\n",
        "            {\n",
        "                \"train_loss\": loss.mean(),\n",
        "                \"learning_rate\": self.lr_schedulers().get_last_lr()[0],\n",
        "            },\n",
        "            step=trainer.global_step,\n",
        "        )\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99vrWKt0Z17x"
      },
      "source": [
        "Now let's define our dataset. This comes straight from MinGPT, and the idea is to serve a sequence of character (of size `block_size`) given any starting point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UXQ-JeOQZ6wR"
      },
      "outputs": [],
      "source": [
        "class CharDataset(Dataset):\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = list(set(data))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        rank_zero_info(\"data has %d characters, %d unique.\" % (data_size, vocab_size))\n",
        "\n",
        "        self.stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "        self.itos = {i: ch for i, ch in enumerate(chars)}\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        chunk = self.data[i : i + self.block_size + 1]\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "\n",
        "        # src and target are off by one, we want the model to predict the next word\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "    def to_tokens(self, message, device):\n",
        "        return torch.tensor([self.stoi[s] for s in message], dtype=torch.long)[\n",
        "            None, ...\n",
        "        ].to(device)\n",
        "\n",
        "    def from_tokens(self, tokens):\n",
        "        return \"\".join([self.itos[int(i)] for i in tokens])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPyC2ig0Z86m"
      },
      "source": [
        "Now, let's plan ahead: how can we probe our model ? Given the training (guess the next character), a nice way is to sample the model given an initial bait. The predictions are then chained after the bait, and we can keep probing the model for predictions over a rolling window. Note that contrary to the training phase, this is sequential, we only predict one character ahead and then repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LUsApc3NaDCP"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
        "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
        "    of block_size, unlike an RNN that has an infinite context window.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    model.eval()\n",
        "\n",
        "    # CREDITS: https://github.com/karpathy/minGPT/blob/master/mingpt/utils.py\n",
        "    def top_k_logits(logits, k):\n",
        "        v, _ = torch.topk(logits, k)\n",
        "        out = logits.clone()\n",
        "        out[out < v[:, [-1]]] = -float(\"Inf\")\n",
        "        return out\n",
        "\n",
        "    for _ in range(steps):\n",
        "        x_cond = (\n",
        "            x if x.size(1) <= block_size else x[:, -block_size:]\n",
        "        )  # crop context if needed\n",
        "        logits = model(x_cond)\n",
        "\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "\n",
        "        # append to the sequence and continue\n",
        "        x = torch.cat((x, ix), dim=1)\n",
        "\n",
        "    return x[0]  # escape the batch dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn3-zIdIaa5Z"
      },
      "source": [
        "Ok, good to go, we're equipped ! Let's train a model. Feel free to alter the parameters to get a feel of what's right or wrong\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O4iejUaQgQcE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "35dc4ee7d02644969777bbdd366d5df2",
            "5bcd02ec86e8469ea55b4133d09a2cd9",
            "8fde90e3f3c74805bf8d5d0286f71897",
            "012882ed0f214445bfe57811e0c16db5",
            "1b71fe17ebff4f908a5077629e2ad7ca",
            "f70c4d028d9549c7bcafa84ea7da4b0b",
            "d920f530ff8f4ccf90103566c2727e16",
            "a8cbd104e5d1460d80b1351437f7a45c",
            "63644eb1af2540aab6bf127dfefdd712",
            "aff3ab87344d4873aa6059c504b4274c",
            "3964e375b9c04bc1847df139ad4ed90e"
          ]
        },
        "outputId": "1d5e91f9-4553-4428-c4d4-9ec1079f47c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_lite.utilities.seed:Global seed set to 42\n",
            "INFO:pytorch_lightning.utilities.rank_zero:data has 1115394 characters, 65 unique.\n",
            "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:441: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n",
            "  rank_zero_deprecation(\n",
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit native Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: /content/lightning_logs\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name  | Type      | Params\n",
            "------------------------------------\n",
            "0 | model | xFormer   | 12.7 M\n",
            "1 | ln_f  | LayerNorm | 1.0 K \n",
            "2 | head  | Linear    | 33.3 K\n",
            "------------------------------------\n",
            "12.7 M    Trainable params\n",
            "0         Non-trainable params\n",
            "12.7 M    Total params\n",
            "25.485    Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35dc4ee7d02644969777bbdd366d5df2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n"
          ]
        }
      ],
      "source": [
        "seed_everything(42)\n",
        "REF_BATCH = 512\n",
        "BATCH = 32  # adjust depending on the avaiable memory on your machine\n",
        "WORKERS = 2\n",
        "EPOCHS = 1\n",
        "BLOCK = 128\n",
        "WARMUP = 20\n",
        "LR = 6e-4\n",
        "LAYERS = 4\n",
        "\n",
        "if not os.path.exists(\"input.txt\"):\n",
        "    os.system(\n",
        "        \"wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "    )\n",
        "\n",
        "text = open(\"input.txt\", \"r\").read()\n",
        "train_dataset = CharDataset(\n",
        "    text, BLOCK\n",
        ")  # one line of poem is roughly 50 characters\n",
        "random_sampler = RandomSampler(train_dataset)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    sampler=random_sampler,\n",
        "    batch_size=BATCH,\n",
        "    num_workers=WORKERS,\n",
        "    pin_memory=True,\n",
        ")\n",
        "\n",
        "model = GPT(\n",
        "    vocab_size=train_dataset.vocab_size,\n",
        "    block_size=train_dataset.block_size,\n",
        "    attention=\"scaled_dot_product\",\n",
        "    warmup_tokens=REF_BATCH * WARMUP,\n",
        "    learning_rate=LR,\n",
        "    final_tokens=EPOCHS * len(train_dataset) * BLOCK,\n",
        "    n_layer=LAYERS\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=1,\n",
        "    max_epochs=EPOCHS,\n",
        "    precision=16,\n",
        "    gradient_clip_val=1,\n",
        "    log_every_n_steps=1,\n",
        "    detect_anomaly=True,\n",
        "    accumulate_grad_batches=REF_BATCH // BATCH,\n",
        ")\n",
        "\n",
        "trainer.fit(model, train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCST-B0REwOD"
      },
      "source": [
        "Alright, this worked ! Let's see what we got\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "w3CDPX9fEycb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba4b495-7305-498e-8835-dcae1f9f1bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Friends of my soul defends all my heart!\n",
            "Thy trumpets encounter this most disdain\n",
            "When he did strike the bosom of the ground,\n",
            "The sea that spiders like to one that would\n",
            "Call it not patient. What shall I go?\n",
            "They are come but thine, that is not wash'd\n",
            "Should so medict in a grumbling tongue.\n",
            "But wherefore would I were some sun in person\n",
            "As you shall have been tricked by his ear,\n",
            "Your suit is hard, that you do strike, and\n",
            "My mind eyes disdainful war and further tried\n",
            "To the match till suppliants with my service\n",
            "Whose fair complaints this mutinous tongue,\n",
            "And with a virtuous passage of the time,\n",
            "Shall have her fint and grand air with a little,\n",
            "The treason of the world report and be absent. Thou,\n",
            "holy father, and hear'st at my work; I would hang thee imperial;\n",
            "which straight were to be bridegroom?\n",
            "Did I nurse? a man; am I strange for hate\n",
            "They have made the messenger-hardening seemed,\n",
            "And his affections wound of his sweet soul.\n",
            "\n",
            "KATHARINA:\n",
            "As I spake i leave: get you home on.\n",
            "Messenger:\n",
            "If you should labour\n"
          ]
        }
      ],
      "source": [
        "context = \"Friends of my soul\"  # Prime with something\n",
        "x = train_dataset.to_tokens(context, model.device)\n",
        "y = sample(model, x, steps=1000, temperature=1.0, sample=True, top_k=10)\n",
        "\n",
        "print(train_dataset.from_tokens(y))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.0 ('xformers')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "35dc4ee7d02644969777bbdd366d5df2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bcd02ec86e8469ea55b4133d09a2cd9",
              "IPY_MODEL_8fde90e3f3c74805bf8d5d0286f71897",
              "IPY_MODEL_012882ed0f214445bfe57811e0c16db5"
            ],
            "layout": "IPY_MODEL_1b71fe17ebff4f908a5077629e2ad7ca"
          }
        },
        "5bcd02ec86e8469ea55b4133d09a2cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f70c4d028d9549c7bcafa84ea7da4b0b",
            "placeholder": "​",
            "style": "IPY_MODEL_d920f530ff8f4ccf90103566c2727e16",
            "value": "Epoch 0: 100%"
          }
        },
        "8fde90e3f3c74805bf8d5d0286f71897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8cbd104e5d1460d80b1351437f7a45c",
            "max": 34853,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63644eb1af2540aab6bf127dfefdd712",
            "value": 34853
          }
        },
        "012882ed0f214445bfe57811e0c16db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aff3ab87344d4873aa6059c504b4274c",
            "placeholder": "​",
            "style": "IPY_MODEL_3964e375b9c04bc1847df139ad4ed90e",
            "value": " 34853/34853 [2:49:11&lt;00:00,  3.43it/s, loss=0.968, v_num=0]"
          }
        },
        "1b71fe17ebff4f908a5077629e2ad7ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "f70c4d028d9549c7bcafa84ea7da4b0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d920f530ff8f4ccf90103566c2727e16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8cbd104e5d1460d80b1351437f7a45c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63644eb1af2540aab6bf127dfefdd712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aff3ab87344d4873aa6059c504b4274c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3964e375b9c04bc1847df139ad4ed90e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}